---
title: "MA678 Homework 2"
date: "9/25/2022"
author: "Su Xu"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#environment set
pacman::p_load("bayesplot","knitr","arm","ggplot2","rstanarm", "dplyr","tidymodels")
```

## 11.5

*Residuals and predictions*: The folder `Pyth` contains outcome $y$ and predictors $x_1$, $x_2$ for 40 data points, with a further 20 points with the predictors but no observed outcome. Save the file to your working directory, then read it into R using `read.table()`.

### (a)

Use R to fit a linear regression model predicting $y$ from $x_1$, $x_2$, using the first 40 data points in the file. Summarize the inferences and check the fit of your model.

```{r}
#read data
pyth <- read.table("../data/ROS-Examples-master/Pyth/pyth.txt", header = T)
#extract first 40 data to fit the linear regression model
pyth1 <- head(pyth, 40)
#fit a linear regression model
pyth1fit <- stan_glm(y ~ x1 + x2, data = pyth1, refresh = 0)
#summary table
summary(pyth1fit)
#comparing data to replications from a fitted model
sims <- as.matrix(pyth1fit)
n_sims <- nrow(sims)
n <- length(pyth1fit$y)
y_rep <- posterior_predict(pyth1fit)
#chekcing model fit using a numerical data summary
test <- function(y) {
  min(y)
}
test_rep <- apply(y_rep,1,test)
#plot a histogram of the minima of the replicated datasets, with a vertical line indicating the minimum of the observed data.
hist(test_rep,
     xlim = range(test(pyth1$y),test_rep))
lines(rep(test(pyth1$y),2),c(0,n))
```

From the histogram plot, we can see that the normal model does capture the variation that Pyth data observed, which can indicate that the model fit the data well.

### (b)

Display the estimated model graphically as in Figure 10.2

```{r}
#data tidy
n_sims <- nrow(as.matrix(pyth1fit))
#first variable
par(mfrow=c(1,2))
plot(pyth1$x1, pyth1$y,
     xlab = "x1",
     ylab = "y")
  #define another variable as its average value
x2_bar <- mean(pyth1$x2)
sims_display <- sample(n_sims, 10)
for (i in sims_display) {
  curve(cbind(1, x2_bar, x) %*% as.matrix(pyth1fit)[i,1:3], 
        add = TRUE,
        lwd = 0.5,
        col = "grey")
}
curve(cbind(1, x2_bar, x) %*% coef(pyth1fit), add = TRUE)

#second variable
plot(pyth1$x2, pyth1$y,
     xlab = "x2",
     ylab = "y")
x1_bar <- mean(pyth1$x1)
for (i in sims_display) {
  curve(cbind(1, x1_bar, x) %*% as.matrix(pyth1fit)[i,1:3],
        add = TRUE,
        lwd = 0.5,
        col = "grey")
}
curve(cbind(1,x1_bar,x) %*% coef(pyth1fit), add = TRUE)
```

### (c)

Make a residual plot for this model. Do the assumptions appear to be met?

```{r}
plot(pyth1fit$linear.predictors,
     pyth1fit$residuals,
     xlab = "linear predictors",
     ylab = "residuals")
abline(0,0)
```

The residual plot shows that residual variables of linear predictors are arounding the line (0,0), which can indicate that the model fit the data well, the assumptions appear to be met.

### (d)

Make predictions for the remaining 20 data points in the file. How confident do you feel about these predictions?

```{r}
pythcoef <- coef(pyth1fit)
pyth2 <- pyth[41:60,]
pyth2[,1] <- rep(pythcoef[1],20) + pythcoef[2]*pyth2[2] + pythcoef[3]*pyth2[3]
pyth2fit <- posterior_predict(pyth1fit,
                              newdata = pyth2)
dim(pyth2fit)
pyth2$predict_mean <- apply(pyth2fit,2,mean)
pyth2$predict_sd <- apply(pyth2fit,2,sd)

ggplot(data = pyth2,
       aes(x = x1)) +
  geom_errorbar(aes(ymin = predict_mean - 2 * predict_sd,
                    ymax = predict_mean + 2 * predict_sd),
                color = "blue") +
  geom_point(aes(y = predict_mean), color = "red") +
  geom_point(aes(y = y), color = "black")
```

The plot shows that the black point and the red point are almost the same, which can make me 99% confident about this model

## 12.5

*Logarithmic transformation and regression*: Consider the following regression: $$\log(\text{weight})=-3.8+2.1 \log(\text{height})+\text{error,} $$ with errors that have standard deviation 0.25. Weights are in pounds and heights are in inches.

### (a)

Fill in the blanks: Approximately 68% of the people will have weights within a factor of \_\_\_\_\_\_ and \_\_\_\_\_\_ of their predicted values from the regression.

```{r}
exp(0.25)
```

#### blank 1: -1.284; blank 2: +1.284 

### (b)

Using pen and paper, sketch the regression line and scatterplot of log(weight) versus log(height) that make sense and are consistent with the fitted model. Be sure to label the axes of your graph.

## 12.6

*Logarithmic transformations*: The folder `Pollution` contains mortality rates and various environmental factors from 60 US metropolitan areas. For this exercise we shall model mortality rate given nitric oxides, sulfur dioxide, and hydrocarbons as inputs. this model is an extreme oversimplication, as it combines all sources of mortality and does not adjust for crucial factors such as age and smoking. We use it to illustrate log transformation in regression.

```{r}
#read data
pollution <- read.csv("../data/ROS-Examples-master/Pollution/data/pollution.csv")
```


### (a)

Create a scatterplot of mortality rate versus level of nitric oxides. Do you think linear regression will fit these data well? Fit the regression and evaluate a residual plot from the regression.

```{r}
#linear regression fit
pollufit <- stan_glm(mort ~ nox, data = pollution, refresh = 0)
#scatter plot
plot(pollution$nox, pollution$mort)
abline(coef(pollufit))
#residual plot
plot(pollufit$linear.predictors,
     pollufit$residuals,
     xlab = "linear predictors",
     ylab = "residuals")
abline(0,0)
```

most of the residual plots are not arounding (0,0), which can indicate that the linear regression model cannot fit the data well.

### (b)

Find an appropriate reansformation that will result in data more appropriate for linear regression. Fit a regression to the transformed data and evaluate the new residual plot.

```{r}
#regression model with logarithmic transformation
pollufit2 <- stan_glm(log(mort) ~ log(nox), data = pollution, refresh = 0)
#scatter plot
plot(log(pollution$nox), log(pollution$mort))
abline(coef(pollufit2))
#residual plot
plot(pollufit2$linear.predictors,
     pollufit2$residuals,
     xlab = "linear predictors",
     ylab = "residuals")
abline(0,0)
```

The log-log model fit the data well. The residual plot is more reasonable than the normal regression model, which indicates that the logarithmic transformed model fit the data better.

### (c)

Interpret the slope coefficient from the model you chose in (b)
```{r}
as.data.frame(cbind(pollufit2$coefficients,pollufit2$ses)) %>% 
  transmute(coefficients = V1,
            ses = V2)
```

for 1% difference in nitric oxides, the predicted difference in mortality rate is 1.595%

### (d)

Now fit a model predicting mortality rate using levels of nitric oxides, sulfur dioxide, and hydrocarbons as inputs. Use appropriate transformation when helpful. Plot the fitted regression model and interpret the coefficients.

```{r}
pollufit3 <- lm(log(mort) ~ log(nox) + log(so2) + log(hc), data = pollution)
summary(pollufit3)
par(mfrow = c(2,2))
plot(pollufit3)
```

for 1% difference in nitric oxides, the predicted positive difference in mortality rate is 5.98%
for 1% difference in sulfur dioxide, the predicted positive difference in mortality rate is 1.43%
for 1% difference in hydrocarbons, the predicted negative difference in mortality rate is 6.08%

### (e)

Cross validate: fit the model you chose above to the first half of the data and then predict for the second half. You used all the data to construct the model in (d), so this is not really cross validation, but it gives a sense of how the steps of cross validation can be implemented.

```{r}
#divide date into two parts
pollution1 <- pollution[1:30,]
pollution2 <- pollution[31:60,]
#fit the model with first half of the data
halffit <- stan_glm(log(mort) ~ log(nox) + log(so2) + log(hc), data = pollution1, refresh = 0)
#leave-one-out cross validation
loo1 <- loo(halffit)
#fit the model with second half of the data
halffit2 <- stan_glm(log(mort) ~ log(nox) + log(so2) + log(hc), data = pollution2, refresh = 0)
loo2 <- loo(halffit2)
loo_compare(loo1, loo2)
```

## 12.7

*Cross validation comparison of models with different transformations of outcomes*: when we compare models with transformed continuous outcomes, we must take into account how the nonlinear transformation warps the continuous outcomes. Follow the procedure used to compare models for the mesquite bushes example on page 202.

### (a)

Compare models for earnings and for log(earnings) given height and sex as shown in page 84 and 192. Use `earnk` and `log(earnk)` as outcomes.

```{r}
#import data
earnings <- read.csv("../data/ROS-Examples-master/Earnings/data/earnings.csv")
#data filter
earnings %>% filter(earn > 0) -> earnings
#model for earnings and log(earnings)
earningfit1 <- stan_glm(earn ~ height + male, data = earnings, refresh = 0)
earningfit2 <- stan_glm(log(earn) ~ height + male, data = earnings, refresh = 0)
#loo cross validations
loo1 <- loo(earningfit1)
loo2 <- loo(earningfit2)
#adjust the predictive comparison with the Jacobian
loo2_with_jacobian <- loo2
#Jacobian adjustment
loo2_with_jacobian$pointwise[,1] <- loo2_with_jacobian$pointwise[,1] - log(earnings$earn)
sum(loo2_with_jacobian$pointwise[,1])
#compare
loo_compare(loo1, loo2_with_jacobian)
```


### (b)

Compare models from other exercises in this chapter.
```{r}
# compare models in 12.6 about mortality rate versus level of nitric oxides and log(mort) vs. log(nox)
loo_1 <- loo(pollufit)
loo_2 <- loo(pollufit2)
loo_compare(loo_1, loo_2)
```


## 12.8

*Log-log transformations*: Suppose that, for a certain population of animals, we can predict log weight from log height as follows:

#### $log(weight)=\beta_0+\beta_1 log(height)+\epsilon$

-   An animal that is 50 centimeters tall is predicted to weigh 10 kg.

#### $log(50)=\beta_0+\beta_1log(10)+\epsilon$

-   Every increase of 1% in height corresponds to a predicted increase of 2% in weight.

#### $log(weight)=\beta_0+0.02 log(height)+\epsilon$

-   The weights of approximately 95% of the animals fall within a factor of 1.1 of predicted values.

$2sd=1.1$

$\sigma = 0.55$

### (a)

Give the equation of the regression line and the residual standard deviation of the regression.
```{r}
#value of $beta_0$
log(50)-(0.02*log(10))-0.55
```

Thus, the equation of the regression line is $log(weight)=3.32+0.02 log(height)+0.55$

The residual standard deviation of the regression is 0.55

### (b)

Suppose the standard deviation of log weights is 20% in this population. What, then, is the $R^{2}$ of the regression model described here?

## 12.9

*Linear and logarithmic transformations*: For a study of congressional elections, you would like a measure of the relative amount of money raised by each of the two major-party candidates in each district. Suppose that you know the amount of money raised by each candidate; label these dollar values $D_i$ and $R_i$. You would like to combine these into a single variable that can be included as an input variable into a model predicting vote share for the Democrats. Discuss the advantages and disadvantages of the following measures:

### (a)

The simple difference, $D_i - R_i$

advantage: simple and no bias

disadvantage: not very accurate

### (b)

The ratio, $D_i / R_i$

advantage: focus more on the comparison of the two samples

disadvantage: it can only illustrate the associations between prior data, lack of environment for further analysis

### (c)

The difference on the logarithmic scale, $\log D_i - \log R_i$

advantage: the coefficients can become more interpretable

disadvantage: not useful if the variables habe narrow dynamic range

### (d)

The relative proportion, $D_{i}/(D_{i}+R_{i})$.

## 12.11

*Elasticity*: An economist runs a regression examining the relations between the average price of cigarettes, $P$, and the quantity purchased, $Q$, across a large sample of counties in the United States, assuming the functional form, $\log Q=\alpha+\beta \log P$. Suppose the estimate for $\beta$ is 0.3. Interpret this coefficient.

The quantity purchased of cigarettes increase by 1%,  the predicted positive difference in the avergae price of cigarettes will be 3%

## 12.13

*Building regression models*: Return to the teaching evaluations data from Exercise 10.6. Fit regression models predicting evaluations given many of the inputs in the dataset. Consider interactions, combinations of predictors, and transformations, as appropriate. Consider several models, discuss in detail the final model that you choose, and also explain why you chose it rather than the others you had considered.

```{r}
#import data
beauty <- read.csv("../data/ROS-Examples-master/Beauty/data/beauty.csv")
#full model
fullmodel <- lm(eval ~ ., data = beauty)
#stepwise model
stepmodel <- stepAIC(fullmodel, direction = "both")
#equation of eval ~ beauty + female + nonenglish + lower gives out the lowest AIC
firstmodel <- lm(eval ~ beauty + female + nonenglish + lower, data = beauty)

#professor's
  #normal linear regression
fit <- lm(eval ~ beauty + female + age, data = beauty)
summary(fit)
plot(fit)
  #the p-value for age is pretty high
  #new fit with interactions of beauty:age, beauty:female
fit1 <- lm(eval ~ beauty * (female + age), data = beauty)
summary(fit1)
  #the new fit shows that age is not reasonable, neither did beauty:female, but it is pretty reasonable as using beauty:age
  #another fit with beauty interacts with all the variables.
fit2 <- lm(eval ~ beauty * female * age, data = beauty)
summary(fit2)

  #ANOVA table
anova(fit,fit1, fit2)

  #T is the square root of F
```

### on-class extention

$H_0: \beta_{beauty}=0.15, \alpha=1\text{%}$

$H_a: \beta_{beauty}\ne 0.15$

t test

$(\hat{\beta_b}-\beta_b)/\hat{SE}(\beta_b)-t_{n,p}=()$

```{r}
s <- summary(fit)$coef
t <- (s[2,1]-0.15)/s[2,2]
t

2 * pt(-abs(t),df.residual(fit))

ta <- qt(0.025, df.residual(fit))
ta #why negative?

s[2,1] + c(-1,1) * -ta *s[2,2]

confint(fit, "beauty")
```

extension 2:

$H_0:\beta_{beauty} + \beta_{female} = 0$

$H_a:\beta_{beauty} + \beta_{female} \ne 0$

using the z test


## 12.14

Prediction from a fitted regression: Consider one of the fitted models for mesquite leaves, for example `fit_4`, in Section 12.6. Suppose you wish to use this model to make inferences about the average mesquite yield in a new set of trees whose predictors are in data frame called new_trees. Give R code to obtain an estimate and standard error for this population average. You do not need to make the prediction; just give the code.

```{r}
#import data
mesquite <- read.table("../data/ROS-Examples-master/Mesquite/data/mesquite.dat", header = T)
#set variables
mesquite$canopy_volume <- mesquite[,3] * mesquite[,4] * mesquite[,6]
mesquite$canopy_area <- mesquite[,3] * mesquite[,4]
mesquite$canopy_shape <- mesquite[,3] / mesquite[,4]
#fit model
fit_4 <- stan_glm(formula = log(weight) ~ log(canopy_volume) + log(canopy_area) + log(canopy_shape) + log(total_height) + log(density) + group, 
                  data = mesquite, 
                  refresh = 0)
# new_trees <- read.table("a path")
# treesfit <- posterior_predict(fit_4, newdata = new_trees)
# dim(treesfit)
# new_trees$predict_mean <- apply(fit_4, 2, mean)
# new_trees$predict_sd <- apply(fit_4, 2, sd)
```


